<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head>
<style>
  body {
    background-color: white;
    padding: 100px;
    width: 1000px;
    margin: auto;
    text-align: left;
    font-weight: 300;
    font-family: 'Open Sans', sans-serif;
    color: #121212;
  }
  h1, h2, h3, h4 {
    font-family: 'Source Sans Pro', sans-serif;
  }
  kbd {
    color: #121212;
  }
</style>
<title>CS 184 Path Tracer</title>
<meta http-equiv="content-type" content="text/html; charset=utf-8" />
<link href="https://fonts.googleapis.com/css?family=Open+Sans|Source+Sans+Pro" rel="stylesheet">

<script>
  MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']]
    }
  };
</script>
<script id="MathJax-script" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js">
</script>

</head>


<body>

<h1 align="middle">CS 184: Computer Graphics and Imaging, Spring 2023</h1>
<h1 align="middle">Project 3-1: Path Tracer</h1>
<h2 align="middle">Ryan Alameddine</h2>

<!-- Add Website URL -->
<h2 align="middle">Website URL: <a href="https://cal-cs184-student.github.io/hw-webpages-sp24-RyanAlameddine/hw3/index.html">https://cal-cs184-student.github.io/hw-webpages-sp24-RyanAlameddine/hw3/index.html</a></h2>

<br><br>


<div align="center">
  <table style="width=100%">
      <tr>
          <td align="middle">
          <img src="images/bunny_adapt.png" width="480px" />
          <figcaption align="middle">Results Caption: my bunny is the bounciest bunny</figcaption>
      </tr>
  </table>
</div>


<div>

<h2 align="middle">Overview</h2>
<p>
    In this project, I implemented a path tracing renderer with a variety of optimizations.
	<br>
	I started off by implementing the primitive operations required for raytracing. This includes generating camera rays, and functions which find intersections between primitives (like triangles and spheres) and rays. At this point, ray tracing to generate an image was incredibly slow because it required testing intersection of every single ray with every single object. 
	<br>
	Next, I implemented the first optimization, the BVH accelleration structure. I constructed a binary tree which partitions the primitives in the scene according to a heuristic I designed. This allows ray-scene intersection tests to be logarithmic in # of objects, not linear. This is because intersection can be calculated by first traversing the tree node bounding boxes which intersect, and only intersecting the ray with the primitives at the leaves.
	<br>
	Equipped with this optimization, I implementation the foundataions for global illumination. I implemented a simple diffuse bsdf which scatters light uniformly across it's hemisphere. Next, I implemented zero-bounce lighting and direct lighting (which sends rays out from scene intersection points to check if that surface should have lighting). First I implemented hemisphere sampling for the bounces, but this caused a lot of noise with low sample counts because most rays would just bounce into nothingness or a non-emissive object. Next, I implemented importance sampling, which sampled directly from the directions which were more likely to hit lights instead. 
	<br>

	Equipped with these tools, I implemented global illumination which is a combination of everything up to this point. Basically, I generate camera rays and intersect them with the scene. If they hit something, they have a chance to bounce again. This happens recursively until the ray misses or a coin flip fails. Each time, we accumulate light which has been building up according to the reflection equation. The final value contributes a sample to the pixel.
	<br>
	Finally, I implemented adaptive sampling, which allowed the pixel sample rate to automatically adjust depending on if we thought the pixel had converged. This sped up renders while allowing for high sampling rate at difficult sections of the image.
	<br>
	In the process of implementing this project, I didn't run into too many issues. The most difficult issues had to do with lighting (images were too bright or too dark in certain sections). By carefully tracing different bounces, I was able to identify that my diffuse BSDF was not correctly normalized, and I was double counting emission on bounces. Other than this, the project went pretty smooth!
</p>
<br>

<h2 align="middle">Part 1: Ray Generation and Scene Intersection (20 Points)</h2>
<!-- Walk through the ray generation and primitive intersection parts of the rendering pipeline.
Explain the triangle intersection algorithm you implemented in your own words.
Show images with normal shading for a few small .dae files. -->

<h3>
  Walk through the ray generation and primitive intersection parts of the rendering pipeline.
</h3>
<p>
    Basically, the goal of this section of the rendering pipeline is to generate pixel samples. For each point in the image space (the space of pixels we are rendering), we want to generate a number of samples to select a color for that pixel. For each sample, I randomly sample a point within corresponding 1x1 pixel grid slot. I transform this point into a ray in world space (by normalizing, converting to camera space, converting to world space, and normalizing/pairing with the camera pos). Next, I check if that ray intersects with anything in the world. If so, I select the color based on the first intersection. The final color for the pixel is the average of all these samples. 
</p>
<br>

<h3>
  Explain the triangle intersection algorithm you implemented in your own words.
</h3>
<p>
    I used Moller Trumbore's algorithm for ray triangle intersection. This algorithm basically solves for $o+t*d=a*p1+b*p2+c*p3$. On the LHS, we see the equation for the ray (origin plus t times the direction). On the RHS, we see barycentric coordinates of a point. Setting them equal allows us to solve for the three unknown using the three equations from the three coordinates of the above (plus the last equation a+b+c=1 from the barycentric invariants). This makes sense because the ray and triangle intersect at the point on the ray which can be represented as a linear combination of the points (which implies the point is on the triangle's plane span). Next, to check if the ray actually intersects the triangle, I check that the barycentric coeffs a,b,c are all greater than or equal to zero to make sure the point is actually inside the triangle. I additionally check that the t value is within the bounds of tmin->tmax.
</p>
<br>

<h3>
  Show images with normal shading for a few small .dae files.
</h3>
<!-- Example of including multiple figures -->
<div align="middle">
  <table style="width:100%">
    <tr align="center">
      <td>
        <img src="images/CBempty.png" align="middle" width="400px"/>
        <figcaption>CBempty.dae</figcaption>
      </td>
      <td>
        <img src="images/CBspheres.png" align="middle" width="400px"/>
        <figcaption>CBspheres.dae</figcaption>
      </td>
    </tr>
    <tr align="center">
      <td>
        <img src="images/CBcoil.png" align="middle" width="400px"/>
        <figcaption>CBcoil.dae</figcaption>
      </td>
      <td>
        <img src="images/CBgems.png" align="middle" width="400px"/>
        <figcaption>CBgems.dae</figcaption>
      </td>
    </tr>
  </table>
</div>
<br>


<h2 align="middle">Part 2: Bounding Volume Hierarchy (20 Points)</h2>
<!-- Walk through your BVH construction algorithm. Explain the heuristic you chose for picking the splitting point.
Show images with normal shading for a few large .dae files that you can only render with BVH acceleration.
Compare rendering times on a few scenes with moderately complex geometries with and without BVH acceleration. Present your results in a one-paragraph analysis. -->

<h3>
  Walk through your BVH construction algorithm. Explain the heuristic you chose for picking the splitting point.
</h3>
<p>
    My BVH construction algorithm is pretty simple. First, I iterate through each primitive within this node. Using this iteration, I collect two things: 1) I expand the node's bounding box using the primitive's bounding box, and 2) I add the centroid of the primitive to an accumulator. Once iteration is complete, I divide the accumulator by the number of elements visited to obtain the average of all the centroids. Next, there are two cases:
	<br>
	If the primitive count is less than or equal to max_leaf_size, then we are done.
	<br>
	Otherwise, we want to split the node again. To do this, I use the following heuristic. I consider the bounding box of this node. Depending on if the width(x), height(y), or length(z) is biggest, I select a normal vector for the splitting plane to be (1, 0, 0), (0, 1, 0), or (0, 0, 1) respectively. Thus, the splitting plane is defined by this vector and the average centroid computed above. I then iterate through each primitive, adding it to a collection of left and right elements depending on whether $(p - c)\cdot n < 0$ (p is the primitive's centroid, c is the average centroid, and n is the normal vector of the split plane).
</p>

<h3>
  Show images with normal shading for a few large .dae files that you can only render with BVH acceleration.
</h3>
<!-- Example of including multiple figures -->
<div align="middle">
  <table style="width:100%">
    <tr align="center">
      <td>
        <img src="images/CBlucy.png" align="middle" width="400px"/>
        <figcaption>CBlucy.dae</figcaption>
      </td>
      <td>
        <img src="images/maxplanck.png" align="middle" width="400px"/>
        <figcaption>maxplanck.dae</figcaption>
      </td>
    </tr>
    <tr align="center">
      <td>
        <img src="images/CBDragon.png" align="middle" width="400px"/>
        <figcaption>CBDragon.dae</figcaption>
      </td>
      <td>
        <img src="images/beast.png" align="middle" width="400px"/>
        <figcaption>beast.dae</figcaption>
      </td>
    </tr>
  </table>
</div>
<br>

<h3>
  Compare rendering times on a few scenes with moderately complex geometries with and without BVH acceleration. Present your results in a one-paragraph analysis.
</h3>
<p>
	Rendering bunny.dae took $.8027 s$ with BVH and $271.7064s$ without. bench.dae took $.8518$ with BVH and $621.2544s$ without. This is multiple orders of magnitude different. This shows how optimizations and accelleration like BVH trees are not just nice additions, but are basically required for any practical ray tracing system with a non-trivial number of primitives.
</p>
<br>

<h2 align="middle">Part 3: Direct Illumination (20 Points)</h2>
<!-- Walk through both implementations of the direct lighting function.
Show some images rendered with both implementations of the direct lighting function.
Focus on one particular scene with at least one area light and compare the noise levels in soft shadows when rendering with 1, 4, 16, and 64 light rays (the -l flag) and with 1 sample per pixel (the -s flag) using light sampling, not uniform hemisphere sampling.
Compare the results between uniform hemisphere sampling and lighting sampling in a one-paragraph analysis. -->

<h3>
  Walk through both implementations of the direct lighting function.
</h3>
<p>
	In each of these functions, I attempt to estimate the direct lighting on a particular point by taking a set of samples. This point is the scene intersection of a camera ray.
	<br>
    Hemisphere:
	I sample a set of directions in a hemisphere around the point. For each sample, I cast a ray from the intersection point in the sampled direction. If it hits an emissive entity (with emission L), I calculate it's contribution to L_out using the reflection equation: $L * bsdf(w_out, w_in) * dot(n, win) * 2*pi$. Note that n is the surface normal and win is w_in in world space. The $2*pi$ term is division by the pdf $1/2pi$, which is required for the monte carlo estimator to be unbiased ($1/2pi$ is the pdf).
	<br>
	I then return the average of all those samples, plus any emission from the surface itself.
	<br>
	Importance:
	For each light in the scene, I sample $ns_area_light$ samples times (or once if it's a point light). I sample from the light itself. I then cast a ray from the hit point to the sampled light direction. If it hits anything, I ignore this sample. If it doesn't, I add the light's contribution to the reflection using the reflection equation $L * bsdf(w_out, w_in) * dot(n, win) /pdf$. Note that this time the pdf is simply divided by directly because pdf is set by the light sampler.
	<br>
	The samples are combined by averaging.
	I return the average of all those samples, plus any emission from the surface itself.
</p>

<h3>
  Show some images rendered with both implementations of the direct lighting function.
</h3>
<!-- Example of including multiple figures -->
<div align="middle">
  <table style="width:100%">
    <!-- Header -->
    <tr align="center">
      <th>
        <b>Uniform Hemisphere Sampling</b>
      </th>
      <th>
        <b>Light Sampling</b>
      </th>
    </tr>
    <br>
    <tr align="center">
      <td>
        <img src="images/bunny_64_32_H.png" align="middle" width="400px"/>
        <figcaption>CBunny.dae</figcaption>
      </td>
      <td>
        <img src="images/bunny_64_32.png" align="middle" width="400px"/>
        <figcaption>CBunny.dae</figcaption>
      </td>
    </tr>
    <br>
    <tr align="center">
      <td>
        <img src="images/CBSpheres_H.png" align="middle" width="400px"/>
        <figcaption>CBspheres_lambertian.dae</figcaption>
      </td>
      <td>
        <img src="images/CBspheres2.png" align="middle" width="400px"/>
        <figcaption>CBspheres_lambertian.dae</figcaption>
      </td>
    </tr>
    <br>
  </table>
</div>
<br>

<h3>
  Focus on one particular scene with at least one area light and compare the noise levels in <b>soft shadows</b> when rendering with 1, 4, 16, and 64 light rays (the -l flag) and with 1 sample per pixel (the -s flag) using light sampling, <b>not</b> uniform hemisphere sampling.
</h3>
<!-- Example of including multiple figures -->
<div align="middle">
  <table style="width:100%">
    <tr align="center">
      <td>
        <img src="images/bunny_64_32_1.png" align="middle" width="200px"/>
        <figcaption>1 Light Ray (CBbunny.dae)</figcaption>
      </td>
      <td>
        <img src="images/bunny_64_32_4.png" align="middle" width="200px"/>
        <figcaption>4 Light Rays (CBbunny.dae)</figcaption>
      </td>
    </tr>
    <tr align="center">
      <td>
        <img src="images/bunny_64_32_16.png" align="middle" width="200px"/>
        <figcaption>16 Light Rays (CBbunny.dae)</figcaption>
      </td>
      <td>
        <img src="images/bunny_64_32_64.png" align="middle" width="200px"/>
        <figcaption>64 Light Rays (CBbunny.dae)</figcaption>
      </td>
    </tr>
  </table>
</div>
<p>
    While there is still noise in the shadow of the 64 light ray image, there is a clear decrease in the amount of noise in the smooth shadow as the light ray count increases. This is due to the fact that, at points along the edge of the shadow, the importance sampling function is going to assign high probabilities to directions which are actually occluded from the light source, causing noise to increase.
</p>
<br>

<h3>
  Compare the results between uniform hemisphere sampling and lighting sampling in a one-paragraph analysis.
</h3>
<p>
    Uniform hemisphere sampling has much more noise that lighting sampling in the above examples. While it is technically possible that lighting sampling can be more noisy (in particular this would be the case if the importance sampling functions were not good, and preferred sampling from low-light areas). However, this was not the case in the example scenes which I showed in the first section, so the noise lowered significantly. This is due to the fact that we are not sampling a ton of samples which don't contribute to the lightness because they don't go anywhere near lights.
</p>
<br>


<h2 align="middle">Part 4: Global Illumination (20 Points)</h2>
<!-- Walk through your implementation of the indirect lighting function.
Show some images rendered with global (direct and indirect) illumination. Use 1024 samples per pixel.
Pick one scene and compare rendered views first with only direct illumination, then only indirect illumination. Use 1024 samples per pixel. (You will have to edit PathTracer::at_least_one_bounce_radiance(...) in your code to generate these views.)
For CBbunny.dae, compare rendered views with max_ray_depth set to 0, 1, 2, 3, and 100 (the -m flag). Use 1024 samples per pixel.
Pick one scene and compare rendered views with various sample-per-pixel rates, including at least 1, 2, 4, 8, 16, 64, and 1024. Use 4 light rays.
You will probably want to use the instructional machines for the above renders in order to not burn up your own computer for hours. -->

<h3>
  Walk through your implementation of the indirect lighting function.
</h3>
<p>
    At a high level, my at_least_one_bounce_radiance function works as follows:
	First, I calculate the direct lighting on the current point using the one_bounce_radiance function. Next, I flip a coin to decide if I should continue recursing (continue probability is 65%). If so, I sample from the bsdf to find a direction to search indirectly. I cast a ray from the intersection point in this sampled direction. I then recursively call at_least_one_bounce_radiance on this new intersection. I calculate the contribution of the return value to the overall lighting using the reflection equation. Notably, I make sure to divide by both the bsdf sampling pdf, and by the continuation probability to ensure that my estimator is unbiased.
</p>
<br>

<h3>
  Show some images rendered with global (direct and indirect) illumination. Use 1024 samples per pixel.
</h3>
<!-- Example of including multiple figures -->
<div align="middle">
  <table style="width:100%">
    <tr align="center">
      <td>
        <img src="images/spheres_global.png" align="middle" width="400px"/>
        <figcaption>CBspheres_lambertian.dae</figcaption>
      </td>
      <td>
        <img src="images/bench_global.png" align="middle" width="400px"/>
        <figcaption>bench.dae</figcaption>
      </td>
    </tr>
  </table>
</div>
<br>

<h3>
  Pick one scene and compare rendered views first with only direct illumination, then only indirect illumination. Use 1024 samples per pixel. (You will have to edit PathTracer::at_least_one_bounce_radiance(...) in your code to generate these views.)
</h3>
<!-- Example of including multiple figures -->
<div align="middle">
  <table style="width:100%">
    <tr align="center">
      <td>
        <img src="images/spheres_direct.png" align="middle" width="400px"/>
        <figcaption>Only direct illumination (CBspheres_lambertian.dae)</figcaption>
      </td>
      <td>
        <img src="images/spheres_indirect.png" align="middle" width="400px"/>
        <figcaption>Only indirect illumination (CBspheres_lambertian.dae)</figcaption>
      </td>
    </tr>
  </table>
</div>
<br>

<h3>
  For CBbunny.dae, compare rendered views with max_ray_depth set to 0, 1, 2, 3, and 100 (the -m flag). Use 1024 samples per pixel.
</h3>
<!-- Example of including multiple figures -->
<div align="middle">
  <table style="width:100%">
    <tr align="center">
      <td>
        <img src="images/bunny0.png" align="middle" width="400px"/>
        <figcaption>max_ray_depth = 0 (CBbunny.dae)</figcaption>
      </td>
      <td>
        <img src="images/bunny1.png" align="middle" width="400px"/>
        <figcaption>max_ray_depth = 1 (CBbunny.dae)</figcaption>
      </td>
    </tr>
    <tr align="center">
      <td>
        <img src="images/bunny2.png" align="middle" width="400px"/>
        <figcaption>max_ray_depth = 2 (CBbunny.dae)</figcaption>
      </td>
      <td>
        <img src="images/bunny3.png" align="middle" width="400px"/>
        <figcaption>max_ray_depth = 3 (CBbunny.dae)</figcaption>
      </td>
    </tr>
    <tr align="center">
      <td>
        <img src="images/bunny100.png" align="middle" width="400px"/>
        <figcaption>max_ray_depth = 100 (CBbunny.dae)</figcaption>
      </td>
    </tr>
  </table>
</div>
<br>
<p>
    At 0, we just see the emission plane.
	At 1, we see the all direct lighting.
	After 1, most of the images look very similar (due to the falloff of light intensity after multiple bounces) with nice ambient lighting.
</p>
<br>

<h3>
  Pick one scene and compare rendered views with various sample-per-pixel rates, including at least 1, 2, 4, 8, 16, 64, and 1024. Use 4 light rays.
</h3>
<!-- Example of including multiple figures -->
<div align="middle">
  <table style="width:100%">
    <tr align="center">
      <td>
        <img src="images/spheres1.png" align="middle" width="400px"/>
        <figcaption>1 sample per pixel (CBspheres_lambertian.dae)</figcaption>
      </td>
      <td>
        <img src="images/spheres2.png" align="middle" width="400px"/>
        <figcaption>2 samples per pixel (CBspheres_lambertian.dae)</figcaption>
      </td>
    </tr>
    <tr align="center">
      <td>
        <img src="images/spheres4.png" align="middle" width="400px"/>
        <figcaption>4 samples per pixel (CBspheres_lambertian.dae)</figcaption>
      </td>
      <td>
        <img src="images/spheres8.png" align="middle" width="400px"/>
        <figcaption>8 samples per pixel (CBspheres_lambertian.dae)</figcaption>
      </td>
    </tr>
    <tr align="center">
      <td>
        <img src="images/spheres16.png" align="middle" width="400px"/>
        <figcaption>16 samples per pixel (CBspheres_lambertian.dae)</figcaption>
      </td>
      <td>
        <img src="images/spheres64.png" align="middle" width="400px"/>
        <figcaption>64 samples per pixel (CBspheres_lambertian.dae)</figcaption>
      </td>
    </tr>
    <tr align="center">
      <td>
        <img src="images/spheres1024.png" align="middle" width="400px"/>
        <figcaption>1024 samples per pixel (CBspheres_lambertian.dae)</figcaption>
      </td>
    </tr>
  </table>
</div>
<br>
<p>
    As we can see, as the number of samples per pixel increases, the noise falls off pretty smoothly but slowly.
</p>
<br>


<h2 align="middle">Part 5: Adaptive Sampling (20 Points)</h2>
<!-- Explain adaptive sampling. Walk through your implementation of the adaptive sampling.
Pick one scene and render it with at least 2048 samples per pixel. Show a good sampling rate image with clearly visible differences in sampling rate over various regions and pixels. Include both your sample rate image, which shows your how your adaptive sampling changes depending on which part of the image you are rendering, and your noise-free rendered result. Use 1 sample per light and at least 5 for max ray depth. -->

<h3>
  Explain adaptive sampling. Walk through your implementation of the adaptive sampling.
</h3>
<p>
    Before adaptive sampling, when generating an image with the pathtracing algorithm, I would generate a uniform number of samples per pixel. I would run the light bouncing algorithm for each sample, and average the result. However, there is an unfortunate trade off here. For some samples in easy parts of the image, the samples converge to the true mean very quickly, so the rest of the samples taken are unneccessarily wasting computation time. However, for complex areas of the scene, we might not have enough samples to eliminate most noise. This means we have to choose between wasting computation time and noise (defined by the section of the image which has the most noise we have to worry about). 
	<br>
	The goal of adaptive sampling is to eliminate this trade off by adaptively sampling until we are pretty sure the samples have converged. At a high level, this is what I implemented: I do `samplesPerBatch` samples in a batch. I then compute $I=1.96*\rho/\sqrt{n}$ and compare $I <= .05 \mu$, where n is the number of samples, $\rho$ is the sqrt of the sample variance, and $\mu$ is the sample mean. This is basically checking if we have converged such that a 95% confidence interval is $\mu \plusminus I$ and that that interval is within our tolerance. Once we are within this interval, we exit early from sampling. 
	<br>
	This allows us to exit early from sampling when we converge quickly, saving computation, while still allowing for high numbers of samples in complex areas.
</p>
<br>

<h3>
  Pick two scenes and render them with at least 2048 samples per pixel. Show a good sampling rate image with clearly visible differences in sampling rate over various regions and pixels. Include both your sample rate image, which shows your how your adaptive sampling changes depending on which part of the image you are rendering, and your noise-free rendered result. Use 1 sample per light and at least 5 for max ray depth.
</h3>
<!-- Example of including multiple figures -->
<div align="middle">
  <table style="width:100%">
    <tr align="center">
      <td>
        <img src="images/bunny_adapt.png" align="middle" width="400px"/>
        <figcaption>Rendered image (CBbunny.dae)</figcaption>
      </td>
      <td>
        <img src="images/bunny_adapt_rate.png" align="middle" width="400px"/>
        <figcaption>Sample rate image (CBbunny.dae)</figcaption>
      </td>
    </tr>
    <tr align="center">
      <td>
        <img src="images/spheres_adapt.png" align="middle" width="400px"/>
        <figcaption>Rendered image (CBspheres_lambertian.dae)</figcaption>
      </td>
      <td>
        <img src="images/spheres_adapt_rate.png" align="middle" width="400px"/>
        <figcaption>Sample rate image (CBspheres_lambertian.dae)</figcaption>
      </td>
    </tr>
  </table>
</div>
<br>


</body>
</html>
